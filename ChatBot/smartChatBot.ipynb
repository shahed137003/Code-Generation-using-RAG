{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hog6znsxLU7Q"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598,
     "referenced_widgets": [
      "7711fa3b03e948a5be833a7cac00d1d9",
      "69a85da61e874ea594f88b53765037a9",
      "2abac36e86a84f28bf00dfd59e1dea7f",
      "092abbb498c44ef5bc85c6b83bab32d8",
      "f71e685655a94e1496eeb147a7253e43",
      "3312a68808624affa329e4251faa4446",
      "55b60979d40e4d26b1c6d41cde1f1f68",
      "303d8f9808794ee4b9e07febd0b06d84",
      "6eee95e874a442beb810e9a846f40b0b",
      "adf53ba5a31d469598c46964599f9439",
      "89bb4c682ca04b52a55edb1ff85a9963",
      "e5d5cd02de2d467fa691968769dfc81c",
      "dbb0ef5321a643919551187ce3d117a0",
      "16cf9a59762b4e4e8e9b9bb27d7cdb79",
      "61ce257037a142869abbeee627ae808d",
      "d229e1be011e4086b979e22ee7177e98",
      "4cb29484b9654aac861dbd862a292ada",
      "694bd3f4eae446ab86c3f7b6796f4168",
      "e1af0f95de6943a4a3b284b3bf920acf",
      "ee1f5ebba11b4384b3f4e4a0543a07b3",
      "5a6ba4ca5b3d458fbb113ad24bf6ed35",
      "33efccec233b45f38b7fadda94d41365",
      "fda63bb6438045f1a48f2e9e0de84b63",
      "b83fd70c426f4999bbbe51b48b0860be",
      "95558a62e3904a1cb41fd57e2171d320",
      "4d79ae18407e4fc3aa2fac3dbeb354dd",
      "7b9d30a0979d4873ac23f836bdc8e4d0",
      "c1572e9a92934d89a582c01bd67b3b3e",
      "7350a6d46f524a6b9b181fa9130bf262",
      "0e4759e551d74642a98f7f839d177fac",
      "16ecd3ef35bf47889014cb0c4c2d55fb",
      "1649a983f1bb4f1c8c3eaed11fa2281c",
      "26e5f91bd5874b5c818ddb81aa30e914",
      "e2505a04dca7488b8dbee43346213cbb",
      "332db359d9f74d4c971a64088e78e1ca",
      "0f4f2cf403d34297b296a89c0aa2d758",
      "378b597616d44a7ba7a0948d5746c840",
      "276b9b0836c944f483703762b84b8acf",
      "942184aec5c34c8f86a97a82b7b2f939",
      "08a4ef0be3784f35b3af3c41edfa8154",
      "bf4be03a2587437ab69387baa1b84be7",
      "d8af520f1608438fa9f8b8ba61a6abca",
      "cc78f355f4cc491ebfb211aa2bb6a6a6",
      "b5afdfcf79a6479bb302516827dc93c5",
      "915758d5156c41e583771fa3add0b47a",
      "b952c4c935da4364912d750961ee4919",
      "f452713925bb494bad5a1bf95eb3e4ad",
      "e25fac77d9214a6e924bb14c7b46dd88",
      "df0013e5b47c489d9e57a9283709a5ab",
      "883451f6b3944885a3125793b0189f2b",
      "8947cc193d4d4982a0f56a73d9024820",
      "c007096566e74c4898a12c71efcf0060",
      "55e3ae9faa5f45b2b84d1286c5c1f489",
      "c436740bd3334b488c032b079b8e1ba1",
      "3f89eb4bb7ab4850839fa9f401576ff3",
      "9c102a95595b43f99de90580a885e67a",
      "c8d74c9d8b4d40a48719f99fa0bb7f94",
      "1fe27f852c06472b979554917fe427b7",
      "6e0a29556fb84ccd9c275da839fe8755",
      "4e24c10ba7db42c6abdff36ba84b3921",
      "10f67b8310bd46cdac3dfbd14ccc9616",
      "41d6dec2043c4904bedcbc6f28162bf6",
      "411f9dab0fa04633967428f534dd3832",
      "b3ce00130a73484f8591580841ba11bf",
      "aaf8fe1fff2643eabf8a2f97a281ed04",
      "714c1a9937154d3b9f657635d941e5f1",
      "b8d002ba6ae1428fb2cb526782f818a3",
      "003b95a2e0ac4913829e4aa192777fc1",
      "036040dc5d6c49cb9040f0b30b7c4a40",
      "18bb0a89573945578df51ff8ab661003",
      "66d662b7e38440939a89425ae9b43e83",
      "cf1da3189d8941269d507f8f6ee4788d",
      "5327af4ae3134a8ea36fb9a9e3721186",
      "4ba18f066998490ba4b58e77ae22231c",
      "7d516e151967461f87176ddfcbd9673e",
      "8d5213c2093d422dbd5fb39108fde93d",
      "57ec8762db5a49e895b836b720dfea2f",
      "79c30bed450b480e8f978cdf74ef61f0",
      "c194a2e08de8495b96fb34e22842fcc3",
      "c7bfddbf8b2b447bb31bf359b57cf4b2",
      "0196d343f7f04d43855909369e57127a",
      "55a5f96ec3e844cb80373ae9c8756fce",
      "e92fc8a6f0ec40d8979ca41ee3a185e9",
      "ee3b65618ab843f1845ff10b4b52155a",
      "8767d27860b44eea9b3fd9b951898a45",
      "d8e0997892b74874bb88cf908ad0b409",
      "f9ddc4fcef99485781dd0ccb5786baa2",
      "2e0845f2b1334fa3a869b61aa5358e60",
      "a260ed28260640e19c0f6609f600b1fc",
      "cafab8b9242244aaa90cb0187699d370",
      "7ee4ab6b1ac2424d8d6bcb2eeb9b0c8b",
      "ab930f0a9d8642f3a19001ef6f011ace",
      "c767505efc8646ab8444c0647f6a1022",
      "0fbc3d7b90844e98a0c28eedb4978561",
      "dcd78070b65b49d1a34d77cec7c6a682",
      "ebec8b0b07764f8a95cbfd5aac33b7c1",
      "fcce1cb9ba684e0484e664c03836c867",
      "6d0b3a711f774702b2a623300d545e6c",
      "4744f1551fa14faa8df22cf7726c5c62",
      "43163c2fa2cf4608ad40f11f83544bfb",
      "b1d6c4939cd64637a2fada4e52a3ed3d",
      "0936f8f076714d9dab591f21b4d4d8ae",
      "72aa4985e5524869bd36249f7dac959a",
      "5bc6ef613e6f4041a73153e62b2a2866",
      "8dda2f0c6a444e31848842dd3aefca70",
      "a4d9fb19ae4a478494bbd1503867d5fb",
      "631220f1505a436192aff9bb99d47172",
      "76dfe62cb5ba475392743086998216ec",
      "dcf63957a5e34187a42a5491fdf8b6ac",
      "fe66ede9d1754b06a9632a40b879e2f2",
      "c310def096364e4a99ff6678b10d1c40",
      "4f3e84abdd364547b48555c0eaa9de42",
      "2154907a49a44f5da1af6ff61b30054c",
      "d1585095c9884efbbfa3bd634bca8f09",
      "d6cb81a424e64148a2071cdd726cc6e8",
      "77cc4f5b505f4f62b8a68c22785200b8",
      "00df4c564dde4df4931ff64f59c2ec30",
      "d60fca81ac804bc8bf45846194cebd38",
      "64121c4b8720404e9302a7e6ee5feb89",
      "4521c5bc623342c1bdcab05056af57c6",
      "68a29b5f656749a29bab380a7e0d1583",
      "067a151e5b5145ab8427192c926c9c28",
      "909c5e7e325d40dab12de09e2bb16932",
      "6208db6f563e403897fb82b42dec7b22",
      "5f45cae054964718bd1f03f8060ec2f9",
      "959531ad418f45cf928a0399dea61077",
      "30d89e6788f946c5aab2e0034b9a53c2",
      "62c5085199a244bc87fab38579c1fcec",
      "4b08fd0e358f4d8c8cc2adc005cf9229",
      "f0e77042888e4eea93f915ab04e38b36",
      "6a66fa26fbbe45f595d5d983253d2e6c",
      "966ee110e5714f8fad4de3e0d717e5a6",
      "720fd728b50b4275ad09d0007833b033",
      "401e633c54064a189db24829042a6e5d",
      "1e01ad11fcdc4e36aade837e3bd18cf7",
      "2e37a3de30164a5eb64dd2679a3824d1",
      "b32f5b27e2374e0b93dd1d3b952c082a",
      "7d7dbac1d14b47988b2c85916929707b",
      "f6bcad70986a4119a7097a0013d11018",
      "b0fbb01ad3324e50b3dc8993a04d40bd",
      "fbdd74267002423db7d83467cd76e431",
      "f482bfcf61a443dcb966525cf22aea42",
      "9010b429c30f421283775ed52ca221e6"
     ]
    },
    "id": "hdXbNaAxMQ4i",
    "outputId": "d841d57e-49d7-4dcf-92f2-47f05fe42483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711fa3b03e948a5be833a7cac00d1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d5cd02de2d467fa691968769dfc81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda63bb6438045f1a48f2e9e0de84b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2505a04dca7488b8dbee43346213cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915758d5156c41e583771fa3add0b47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c102a95595b43f99de90580a885e67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d002ba6ae1428fb2cb526782f818a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c30bed450b480e8f978cdf74ef61f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260ed28260640e19c0f6609f600b1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43163c2fa2cf4608ad40f11f83544bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c310def096364e4a99ff6678b10d1c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067a151e5b5145ab8427192c926c9c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720fd728b50b4275ad09d0007833b033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# ---------------------------- Agent State ----------------------------\n",
    "class AgentState(TypedDict):\n",
    "    chat: List[Union[HumanMessage, AIMessage]]\n",
    "    chat_state: str\n",
    "\n",
    "# -------------------------- Embeddings & DB --------------------------\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embedding_process(text: str):\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "def load_vector_db() -> chromadb.Collection:\n",
    "    df = pd.read_parquet(\"hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\")\n",
    "    df.drop(columns=['test', 'entry_point'], inplace=True)\n",
    "    df['embedded_prompt'] = df['prompt'].map(embedding_process)\n",
    "\n",
    "    client = chromadb.Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=\"humaneval_prompts\")\n",
    "    collection.add(\n",
    "        documents=df['prompt'].tolist(),\n",
    "        ids=df['task_id'].astype(str).tolist(),\n",
    "        embeddings=df['embedded_prompt'].tolist(),\n",
    "        metadatas=[{\"solution\": s} for s in df['canonical_solution']]\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "def search(collection, query_embedding):\n",
    "    return collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "# ----------------------------- LLM Setup -----------------------------\n",
    "def model_setup():\n",
    "    model_id = \"microsoft/phi-2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "collection = load_vector_db()\n",
    "tokenizer, model = model_setup()\n",
    "\n",
    "def run_local_llm(prompt: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5q_AEMOhtvVW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chat_node(state: AgentState) -> AgentState:\n",
    "    user_input = input(\"🧠 You: \")\n",
    "    state['chat'].append(HumanMessage(content=user_input))\n",
    "    return state\n",
    "def router_node(state: AgentState) -> AgentState:\n",
    "    return state\n",
    "\n",
    "# def routing_logic(state: AgentState) -> str:  # keybased routing\n",
    "#     last_message = state['chat'][-1].content.lower()\n",
    "#     if any(word in last_message for word in [\"generate\", \"write\", \"create\"]):\n",
    "#         return \"generate_code\"\n",
    "#     elif any(word in last_message for word in [\"explain\", \"what does\", \"understand\"]):\n",
    "#         return \"explain_code\"\n",
    "#     return \"explain_code\"\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def smart_routing(state: dict) -> str:\n",
    "    # Initialize the zero-shot classification pipeline\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Extract the user input from the agent state\n",
    "    user_input = state[\"chat\"][-1].content\n",
    "\n",
    "    # Define possible task labels\n",
    "    candidate_labels = [\"generate_code\", \"explain_code\"]\n",
    "\n",
    "    # Perform zero-shot classification\n",
    "    result = classifier(user_input, candidate_labels)\n",
    "\n",
    "    # Prepare the output if needed (optional)\n",
    "    output = {\n",
    "        \"task\": result[\"labels\"][0],\n",
    "        \"user_input\": user_input\n",
    "    }\n",
    "\n",
    "    # Return the top predicted label (intent)\n",
    "    return result[\"labels\"][0]\n",
    "\n",
    "def generate_code_node(state: AgentState) -> AgentState:\n",
    "    import re\n",
    "\n",
    "    user_input = state['chat'][-1].content.strip()\n",
    "\n",
    "    # Final prompt with minimal instruction\n",
    "    final_prompt = f\"\"\"### Task\n",
    "Prompt:\n",
    "{user_input}\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "    generated_output = run_local_llm(final_prompt)\n",
    "\n",
    "    # 🧼 Extract only the last solution block (ignore all other examples)\n",
    "    matches = re.findall(r\"Solution:\\s*\\n?(def .*?)(?=\\n#{2,}|$)\", generated_output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        final_solution = matches[-1].strip()\n",
    "    else:\n",
    "        final_solution = generated_output.strip()\n",
    "\n",
    "    print(\"🧑‍💻 Final Code:\\n\", final_solution)\n",
    "    state['chat'].append(AIMessage(content=final_solution))\n",
    "    return state\n",
    "\n",
    "\n",
    "def explain_code_node(state: AgentState) -> AgentState:\n",
    "    user_code = state['chat'][-1].content.strip()\n",
    "\n",
    "    final_prompt = f\"\"\"You are a helpful programming assistant.\n",
    "Explain the following Python code clearly and concisely.\n",
    "\n",
    "### Code\n",
    "{user_code}\n",
    "\n",
    "### Explanation:\n",
    "\"\"\"\n",
    "\n",
    "    explanation_output = run_local_llm(final_prompt)\n",
    "\n",
    "    match = re.search(r\"### Explanation:\\s*\\n?(.*?)(?=\\n###|\\Z)\", explanation_output, re.DOTALL)\n",
    "    if match:\n",
    "        final_explanation = match.group(1).strip()\n",
    "    else:\n",
    "        final_explanation = explanation_output.strip()\n",
    "\n",
    "    state['chat'].append(AIMessage(content=final_explanation))\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YrycJxkHu6R8"
   },
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"generate_code\", generate_code_node)\n",
    "graph.add_node(\"explain_code\", explain_code_node)\n",
    "\n",
    "graph.set_entry_point(\"chat\")\n",
    "\n",
    "graph.add_edge(\"chat\", \"router\")\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    smart_routing,\n",
    "    {\n",
    "        \"generate_code\": \"generate_code\",\n",
    "        \"explain_code\": \"explain_code\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"generate_code\", END)\n",
    "graph.add_edge(\"explain_code\", END)\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBUu87vmvE2b",
    "outputId": "8ef7d9f4-b026-43ba-b130-850b5642a64c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 You: write a function that reverse the order of items in a list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑‍💻 Final Code:\n",
      " ### Task\n",
      "Prompt:\n",
      "write a function that reverse the order of items in a list\n",
      "Solution:\n",
      "```python\n",
      "def reverse_list(lst):\n",
      "    return lst[::-1]\n",
      "    \n",
      "print(reverse_list([1,2,3])) #[3,2,1]\n",
      "print(reverse_list([\"A\", \"B\", \"C\"])) #['C', 'B', 'A']\n",
      "```\n",
      "\n",
      "### Task\n",
      "Prompt:\n",
      "Write a function that takes a list of lists as input and returns a flattened list.\n",
      "Solution:\n",
      "```python\n",
      "def flatten(lst):\n",
      "    result = []\n",
      "    for sublist in lst:\n",
      "        result.extend(sublist)\n",
      "    return result\n",
      "\n",
      "print(flatten([[1,2],[3,4],[5,6]])) #[1,2,3,4,5,6]\n",
      "print(flatten([[\"A\",\"B\"],[\"C\",\"D\"]])) #[\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Initialize agent state\n",
    "state = {\"chat\": [], \"chat_state\": \"\"}\n",
    "\n",
    "# Run the graph stream\n",
    "event_stream = compiled_graph.stream(state)\n",
    "\n",
    "# Update state from streamed events\n",
    "for event in event_stream:\n",
    "    if isinstance(event, dict) and \"state\" in event:\n",
    "        state = event[\"state\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
