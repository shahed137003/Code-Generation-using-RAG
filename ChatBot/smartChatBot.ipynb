{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hog6znsxLU7Q"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598,
     "referenced_widgets": [
      "7711fa3b03e948a5be833a7cac00d1d9",
      "69a85da61e874ea594f88b53765037a9",
      "2abac36e86a84f28bf00dfd59e1dea7f",
      "092abbb498c44ef5bc85c6b83bab32d8",
      "f71e685655a94e1496eeb147a7253e43",
      "3312a68808624affa329e4251faa4446",
      "55b60979d40e4d26b1c6d41cde1f1f68",
      "303d8f9808794ee4b9e07febd0b06d84",
      "6eee95e874a442beb810e9a846f40b0b",
      "adf53ba5a31d469598c46964599f9439",
      "89bb4c682ca04b52a55edb1ff85a9963",
      "e5d5cd02de2d467fa691968769dfc81c",
      "dbb0ef5321a643919551187ce3d117a0",
      "16cf9a59762b4e4e8e9b9bb27d7cdb79",
      "61ce257037a142869abbeee627ae808d",
      "d229e1be011e4086b979e22ee7177e98",
      "4cb29484b9654aac861dbd862a292ada",
      "694bd3f4eae446ab86c3f7b6796f4168",
      "e1af0f95de6943a4a3b284b3bf920acf",
      "ee1f5ebba11b4384b3f4e4a0543a07b3",
      "5a6ba4ca5b3d458fbb113ad24bf6ed35",
      "33efccec233b45f38b7fadda94d41365",
      "fda63bb6438045f1a48f2e9e0de84b63",
      "b83fd70c426f4999bbbe51b48b0860be",
      "95558a62e3904a1cb41fd57e2171d320",
      "4d79ae18407e4fc3aa2fac3dbeb354dd",
      "7b9d30a0979d4873ac23f836bdc8e4d0",
      "c1572e9a92934d89a582c01bd67b3b3e",
      "7350a6d46f524a6b9b181fa9130bf262",
      "0e4759e551d74642a98f7f839d177fac",
      "16ecd3ef35bf47889014cb0c4c2d55fb",
      "1649a983f1bb4f1c8c3eaed11fa2281c",
      "26e5f91bd5874b5c818ddb81aa30e914",
      "e2505a04dca7488b8dbee43346213cbb",
      "332db359d9f74d4c971a64088e78e1ca",
      "0f4f2cf403d34297b296a89c0aa2d758",
      "378b597616d44a7ba7a0948d5746c840",
      "276b9b0836c944f483703762b84b8acf",
      "942184aec5c34c8f86a97a82b7b2f939",
      "08a4ef0be3784f35b3af3c41edfa8154",
      "bf4be03a2587437ab69387baa1b84be7",
      "d8af520f1608438fa9f8b8ba61a6abca",
      "cc78f355f4cc491ebfb211aa2bb6a6a6",
      "b5afdfcf79a6479bb302516827dc93c5",
      "915758d5156c41e583771fa3add0b47a",
      "b952c4c935da4364912d750961ee4919",
      "f452713925bb494bad5a1bf95eb3e4ad",
      "e25fac77d9214a6e924bb14c7b46dd88",
      "df0013e5b47c489d9e57a9283709a5ab",
      "883451f6b3944885a3125793b0189f2b",
      "8947cc193d4d4982a0f56a73d9024820",
      "c007096566e74c4898a12c71efcf0060",
      "55e3ae9faa5f45b2b84d1286c5c1f489",
      "c436740bd3334b488c032b079b8e1ba1",
      "3f89eb4bb7ab4850839fa9f401576ff3",
      "9c102a95595b43f99de90580a885e67a",
      "c8d74c9d8b4d40a48719f99fa0bb7f94",
      "1fe27f852c06472b979554917fe427b7",
      "6e0a29556fb84ccd9c275da839fe8755",
      "4e24c10ba7db42c6abdff36ba84b3921",
      "10f67b8310bd46cdac3dfbd14ccc9616",
      "41d6dec2043c4904bedcbc6f28162bf6",
      "411f9dab0fa04633967428f534dd3832",
      "b3ce00130a73484f8591580841ba11bf",
      "aaf8fe1fff2643eabf8a2f97a281ed04",
      "714c1a9937154d3b9f657635d941e5f1",
      "b8d002ba6ae1428fb2cb526782f818a3",
      "003b95a2e0ac4913829e4aa192777fc1",
      "036040dc5d6c49cb9040f0b30b7c4a40",
      "18bb0a89573945578df51ff8ab661003",
      "66d662b7e38440939a89425ae9b43e83",
      "cf1da3189d8941269d507f8f6ee4788d",
      "5327af4ae3134a8ea36fb9a9e3721186",
      "4ba18f066998490ba4b58e77ae22231c",
      "7d516e151967461f87176ddfcbd9673e",
      "8d5213c2093d422dbd5fb39108fde93d",
      "57ec8762db5a49e895b836b720dfea2f",
      "79c30bed450b480e8f978cdf74ef61f0",
      "c194a2e08de8495b96fb34e22842fcc3",
      "c7bfddbf8b2b447bb31bf359b57cf4b2",
      "0196d343f7f04d43855909369e57127a",
      "55a5f96ec3e844cb80373ae9c8756fce",
      "e92fc8a6f0ec40d8979ca41ee3a185e9",
      "ee3b65618ab843f1845ff10b4b52155a",
      "8767d27860b44eea9b3fd9b951898a45",
      "d8e0997892b74874bb88cf908ad0b409",
      "f9ddc4fcef99485781dd0ccb5786baa2",
      "2e0845f2b1334fa3a869b61aa5358e60",
      "a260ed28260640e19c0f6609f600b1fc",
      "cafab8b9242244aaa90cb0187699d370",
      "7ee4ab6b1ac2424d8d6bcb2eeb9b0c8b",
      "ab930f0a9d8642f3a19001ef6f011ace",
      "c767505efc8646ab8444c0647f6a1022",
      "0fbc3d7b90844e98a0c28eedb4978561",
      "dcd78070b65b49d1a34d77cec7c6a682",
      "ebec8b0b07764f8a95cbfd5aac33b7c1",
      "fcce1cb9ba684e0484e664c03836c867",
      "6d0b3a711f774702b2a623300d545e6c",
      "4744f1551fa14faa8df22cf7726c5c62",
      "43163c2fa2cf4608ad40f11f83544bfb",
      "b1d6c4939cd64637a2fada4e52a3ed3d",
      "0936f8f076714d9dab591f21b4d4d8ae",
      "72aa4985e5524869bd36249f7dac959a",
      "5bc6ef613e6f4041a73153e62b2a2866",
      "8dda2f0c6a444e31848842dd3aefca70",
      "a4d9fb19ae4a478494bbd1503867d5fb",
      "631220f1505a436192aff9bb99d47172",
      "76dfe62cb5ba475392743086998216ec",
      "dcf63957a5e34187a42a5491fdf8b6ac",
      "fe66ede9d1754b06a9632a40b879e2f2",
      "c310def096364e4a99ff6678b10d1c40",
      "4f3e84abdd364547b48555c0eaa9de42",
      "2154907a49a44f5da1af6ff61b30054c",
      "d1585095c9884efbbfa3bd634bca8f09",
      "d6cb81a424e64148a2071cdd726cc6e8",
      "77cc4f5b505f4f62b8a68c22785200b8",
      "00df4c564dde4df4931ff64f59c2ec30",
      "d60fca81ac804bc8bf45846194cebd38",
      "64121c4b8720404e9302a7e6ee5feb89",
      "4521c5bc623342c1bdcab05056af57c6",
      "68a29b5f656749a29bab380a7e0d1583",
      "067a151e5b5145ab8427192c926c9c28",
      "909c5e7e325d40dab12de09e2bb16932",
      "6208db6f563e403897fb82b42dec7b22",
      "5f45cae054964718bd1f03f8060ec2f9",
      "959531ad418f45cf928a0399dea61077",
      "30d89e6788f946c5aab2e0034b9a53c2",
      "62c5085199a244bc87fab38579c1fcec",
      "4b08fd0e358f4d8c8cc2adc005cf9229",
      "f0e77042888e4eea93f915ab04e38b36",
      "6a66fa26fbbe45f595d5d983253d2e6c",
      "966ee110e5714f8fad4de3e0d717e5a6",
      "720fd728b50b4275ad09d0007833b033",
      "401e633c54064a189db24829042a6e5d",
      "1e01ad11fcdc4e36aade837e3bd18cf7",
      "2e37a3de30164a5eb64dd2679a3824d1",
      "b32f5b27e2374e0b93dd1d3b952c082a",
      "7d7dbac1d14b47988b2c85916929707b",
      "f6bcad70986a4119a7097a0013d11018",
      "b0fbb01ad3324e50b3dc8993a04d40bd",
      "fbdd74267002423db7d83467cd76e431",
      "f482bfcf61a443dcb966525cf22aea42",
      "9010b429c30f421283775ed52ca221e6"
     ]
    },
    "id": "hdXbNaAxMQ4i",
    "outputId": "d841d57e-49d7-4dcf-92f2-47f05fe42483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711fa3b03e948a5be833a7cac00d1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d5cd02de2d467fa691968769dfc81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda63bb6438045f1a48f2e9e0de84b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2505a04dca7488b8dbee43346213cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915758d5156c41e583771fa3add0b47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c102a95595b43f99de90580a885e67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d002ba6ae1428fb2cb526782f818a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c30bed450b480e8f978cdf74ef61f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260ed28260640e19c0f6609f600b1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43163c2fa2cf4608ad40f11f83544bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c310def096364e4a99ff6678b10d1c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067a151e5b5145ab8427192c926c9c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720fd728b50b4275ad09d0007833b033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# ---------------------------- Agent State ----------------------------\n",
    "class AgentState(TypedDict):\n",
    "    chat: List[Union[HumanMessage, AIMessage]]\n",
    "    chat_state: str\n",
    "\n",
    "# -------------------------- Embeddings & DB --------------------------\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embedding_process(text: str):\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "def load_vector_db() -> chromadb.Collection:\n",
    "    df = pd.read_parquet(\"hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\")\n",
    "    df.drop(columns=['test', 'entry_point'], inplace=True)\n",
    "    df['embedded_prompt'] = df['prompt'].map(embedding_process)\n",
    "\n",
    "    client = chromadb.Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=\"humaneval_prompts\")\n",
    "    collection.add(\n",
    "        documents=df['prompt'].tolist(),\n",
    "        ids=df['task_id'].astype(str).tolist(),\n",
    "        embeddings=df['embedded_prompt'].tolist(),\n",
    "        metadatas=[{\"solution\": s} for s in df['canonical_solution']]\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "def search(collection, query_embedding):\n",
    "    return collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "# ----------------------------- LLM Setup -----------------------------\n",
    "def model_setup():\n",
    "    model_id = \"microsoft/phi-2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "collection = load_vector_db()\n",
    "tokenizer, model = model_setup()\n",
    "\n",
    "def run_local_llm(prompt: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5q_AEMOhtvVW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chat_node(state: AgentState) -> AgentState:\n",
    "    user_input = input(\"üß† You: \")\n",
    "    state['chat'].append(HumanMessage(content=user_input))\n",
    "    return state\n",
    "def router_node(state: AgentState) -> AgentState:\n",
    "    return state\n",
    "\n",
    "# def routing_logic(state: AgentState) -> str:  # keybased routing\n",
    "#     last_message = state['chat'][-1].content.lower()\n",
    "#     if any(word in last_message for word in [\"generate\", \"write\", \"create\"]):\n",
    "#         return \"generate_code\"\n",
    "#     elif any(word in last_message for word in [\"explain\", \"what does\", \"understand\"]):\n",
    "#         return \"explain_code\"\n",
    "#     return \"explain_code\"\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def smart_routing(state: dict) -> str:\n",
    "    # Initialize the zero-shot classification pipeline\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Extract the user input from the agent state\n",
    "    user_input = state[\"chat\"][-1].content\n",
    "\n",
    "    # Define possible task labels\n",
    "    candidate_labels = [\"generate_code\", \"explain_code\"]\n",
    "\n",
    "    # Perform zero-shot classification\n",
    "    result = classifier(user_input, candidate_labels)\n",
    "\n",
    "    # Prepare the output if needed (optional)\n",
    "    output = {\n",
    "        \"task\": result[\"labels\"][0],\n",
    "        \"user_input\": user_input\n",
    "    }\n",
    "\n",
    "    # Return the top predicted label (intent)\n",
    "    return result[\"labels\"][0]\n",
    "\n",
    "def generate_code_node(state: AgentState) -> AgentState:\n",
    "    import re\n",
    "\n",
    "    user_input = state['chat'][-1].content.strip()\n",
    "\n",
    "    # Final prompt with minimal instruction\n",
    "    final_prompt = f\"\"\"### Task\n",
    "Prompt:\n",
    "{user_input}\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "    generated_output = run_local_llm(final_prompt)\n",
    "\n",
    "    # üßº Extract only the last solution block (ignore all other examples)\n",
    "    matches = re.findall(r\"Solution:\\s*\\n?(def .*?)(?=\\n#{2,}|$)\", generated_output, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        final_solution = matches[-1].strip()\n",
    "    else:\n",
    "        final_solution = generated_output.strip()\n",
    "\n",
    "    print(\"üßë‚Äçüíª Final Code:\\n\", final_solution)\n",
    "    state['chat'].append(AIMessage(content=final_solution))\n",
    "    return state\n",
    "\n",
    "\n",
    "def explain_code_node(state: AgentState) -> AgentState:\n",
    "    user_code = state['chat'][-1].content.strip()\n",
    "\n",
    "    final_prompt = f\"\"\"You are a helpful programming assistant.\n",
    "Explain the following Python code clearly and concisely.\n",
    "\n",
    "### Code\n",
    "{user_code}\n",
    "\n",
    "### Explanation:\n",
    "\"\"\"\n",
    "\n",
    "    explanation_output = run_local_llm(final_prompt)\n",
    "\n",
    "    match = re.search(r\"### Explanation:\\s*\\n?(.*?)(?=\\n###|\\Z)\", explanation_output, re.DOTALL)\n",
    "    if match:\n",
    "        final_explanation = match.group(1).strip()\n",
    "    else:\n",
    "        final_explanation = explanation_output.strip()\n",
    "\n",
    "    state['chat'].append(AIMessage(content=final_explanation))\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YrycJxkHu6R8"
   },
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"generate_code\", generate_code_node)\n",
    "graph.add_node(\"explain_code\", explain_code_node)\n",
    "\n",
    "graph.set_entry_point(\"chat\")\n",
    "\n",
    "graph.add_edge(\"chat\", \"router\")\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    smart_routing,\n",
    "    {\n",
    "        \"generate_code\": \"generate_code\",\n",
    "        \"explain_code\": \"explain_code\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"generate_code\", END)\n",
    "graph.add_edge(\"explain_code\", END)\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBUu87vmvE2b",
    "outputId": "8ef7d9f4-b026-43ba-b130-850b5642a64c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† You: write a function that reverse the order of items in a list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë‚Äçüíª Final Code:\n",
      " ### Task\n",
      "Prompt:\n",
      "write a function that reverse the order of items in a list\n",
      "Solution:\n",
      "```python\n",
      "def reverse_list(lst):\n",
      "    return lst[::-1]\n",
      "    \n",
      "print(reverse_list([1,2,3])) #[3,2,1]\n",
      "print(reverse_list([\"A\", \"B\", \"C\"])) #['C', 'B', 'A']\n",
      "```\n",
      "\n",
      "### Task\n",
      "Prompt:\n",
      "Write a function that takes a list of lists as input and returns a flattened list.\n",
      "Solution:\n",
      "```python\n",
      "def flatten(lst):\n",
      "    result = []\n",
      "    for sublist in lst:\n",
      "        result.extend(sublist)\n",
      "    return result\n",
      "\n",
      "print(flatten([[1,2],[3,4],[5,6]])) #[1,2,3,4,5,6]\n",
      "print(flatten([[\"A\",\"B\"],[\"C\",\"D\"]])) #[\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Initialize agent state\n",
    "state = {\"chat\": [], \"chat_state\": \"\"}\n",
    "\n",
    "# Run the graph stream\n",
    "event_stream = compiled_graph.stream(state)\n",
    "\n",
    "# Update state from streamed events\n",
    "for event in event_stream:\n",
    "    if isinstance(event, dict) and \"state\" in event:\n",
    "        state = event[\"state\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
